from flask import Flask, render_template, jsonify
from flask_cors import CORS
import feedparser
import requests
from datetime import datetime, timedelta
# from googletrans import Translator  # Kikomment√°lva - AI-val ford√≠tunk
import hashlib
import os
from dotenv import load_dotenv
import schedule
import threading
import time
from ai_processor import GovernmentEconomicAnalyzer

load_dotenv()

app = Flask(__name__)
CORS(app)

# Glob√°lis v√°ltoz√≥k a h√≠rek t√°rol√°s√°ra
newsletter_data = {
    'articles': [],
    'executive_briefing': '',
    'last_update': None,
    'update_count': 0,
    'processing_status': 'idle'
}

# AI elemz≈ë inicializ√°l√°sa
ai_analyzer = GovernmentEconomicAnalyzer()

# Gazdas√°gi RSS forr√°sok (angol nyelv≈±)
ECONOMIC_SOURCES = [
    # PR√âMIUM FORR√ÅSOK
    {
        'name': 'Bloomberg Markets',
        'url': 'https://feeds.bloomberg.com/markets/news.rss',
        'category': 'Piacok'
    },
    {
        'name': 'Bloomberg Politics',
        'url': 'https://feeds.bloomberg.com/politics/news.rss',
        'category': 'Gazdas√°gpolitika'
    },
    {
        'name': 'Bloomberg Technology',
        'url': 'https://feeds.bloomberg.com/technology/news.rss',
        'category': 'Tech & Gazdas√°g'
    },
    {
        'name': 'Financial Times',
        'url': 'https://www.ft.com/rss/home',
        'category': '√Åltal√°nos gazdas√°g'
    },
    {
        'name': 'The Economist - Finance',
        'url': 'https://www.economist.com/finance-and-economics/rss.xml',
        'category': 'Elemz√©sek'
    },
    {
        'name': 'The Economist - Business',
        'url': 'https://www.economist.com/business/rss.xml',
        'category': '√úzleti elemz√©sek'
    },
    {
        'name': 'Reuters Business',
        'url': 'https://feeds.reuters.com/reuters/businessNews',
        'category': '√úzleti h√≠rek'
    },
    {
        'name': 'Wall Street Journal',
        'url': 'https://feeds.wsj.com/rss/RSSWorldNews.xml',
        'category': 'P√©nz√ºgyek'
    },
    {
        'name': 'MarketWatch',
        'url': 'https://feeds.content.dowjones.io/public/rss/mw_topstories',
        'category': 'T≈ëzsde'
    },
    {
        'name': 'CNBC',
        'url': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
        'category': 'T≈ëzsde'
    },
    {
        'name': 'Business Insider',
        'url': 'https://markets.businessinsider.com/rss/news',
        'category': 'Piacok & √úzlet'
    },
    {
        'name': 'BBC Business',
        'url': 'http://feeds.bbci.co.uk/news/business/rss.xml',
        'category': 'Glob√°lis gazdas√°g'
    },
    # ELEMZ√âSEK & THINK TANK
    {
        'name': 'Investopedia - V√°llalati',
        'url': 'https://www.investopedia.com/feedbuilder/feed/getfeed?feedName=rss_headline&categoryName=company-news',
        'category': 'V√°llalati h√≠rek'
    },
    {
        'name': 'Investopedia - Piacok',
        'url': 'https://www.investopedia.com/feedbuilder/feed/getfeed?feedName=rss_headline&categoryName=markets-news',
        'category': 'Piaci h√≠rek'
    },
    {
        'name': 'Economic Policy Institute',
        'url': 'http://feeds.feedburner.com/epi',
        'category': 'Gazdas√°gpolitika'
    },
    {
        'name': 'Federal Reserve FRED',
        'url': 'https://fredblog.stlouisfed.org/feed/',
        'category': 'Fed elemz√©sek'
    },
    {
        'name': 'Congressional Budget Office',
        'url': 'https://www.cbo.gov/publications/all/rss.xml',
        'category': 'K√∂lts√©gvet√©s'
    },
    # BEFOLY√ÅSOS BLOGOK
    {
        'name': 'Calculated Risk',
        'url': 'http://feeds.feedburner.com/calculatedrisk',
        'category': 'Makrogazdas√°g'
    },
    {
        'name': 'Marginal Revolution',
        'url': 'http://feeds.feedburner.com/marginalrevolution',
        'category': 'K√∂zgazdas√°gtan'
    },
    {
        'name': 'Financial Samurai',
        'url': 'https://financialsamurai.com/feed/',
        'category': 'P√©nz√ºgyi tan√°csok'
    },
    {
        'name': 'Zero Hedge',
        'url': 'http://feeds.feedburner.com/zerohedge/feed',
        'category': 'Alternat√≠v elemz√©s'
    }
]

# translator = Translator()  # Kikomment√°lva - AI-val ford√≠tunk

def generate_article_id(title, source):
    """Egyedi azonos√≠t√≥ gener√°l√°sa"""
    content = f"{title}-{source}".encode('utf-8')
    return hashlib.md5(content).hexdigest()[:12]

def translate_text(text, max_retries=3):
    """Sz√∂veg ford√≠t√°sa angol->magyar AI-val"""
    if not text:
        return text
    
    # Haszn√°ljuk a Gemini API-t a ford√≠t√°shoz
    try:
        if ai_analyzer.gemini_model:
            prompt = f"Translate to Hungarian (output ONLY the Hungarian translation, no explanations): {text}"
            response = ai_analyzer.gemini_model.generate_content(prompt)
            # Clean up response - remove any THOUGHT sections or extra text
            result = response.text.strip()
            if "THOUGHT:" in result:
                # Extract only the translation part
                lines = result.split('\n')
                for line in lines:
                    if line.strip() and not line.startswith('THOUGHT:') and not line.startswith('*'):
                        return line.strip()
            return result
        else:
            # Ha nincs Gemini API, adjuk vissza az angol sz√∂veget
            return text
    except Exception as e:
        print(f"Ford√≠t√°si hiba: {e}")
        return text

def fetch_and_process_news():
    """H√≠rek lek√©r√©se √©s feldolgoz√°sa korm√°nyzati elemz√©ssel"""
    newsletter_data['processing_status'] = 'processing'
    print(f"\n{'='*60}")
    print(f"üèõÔ∏è KORM√ÅNYZATI GAZDAS√ÅGI H√çRLEV√âL FRISS√çT√âSE")
    print(f"Id≈ëpont: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")
    
    all_articles = []
    
    # RSS forr√°sok feldolgoz√°sa
    for source in ECONOMIC_SOURCES:
        try:
            print(f"üì° Lek√©r√©s: {source['name']}")
            feed = feedparser.parse(source['url'])
            
            for entry in feed.entries[:3]:  # Max 3 cikk forr√°sonk√©nt a min≈ës√©g miatt
                # Alapadatok kinyer√©se
                title = entry.get('title', 'Nincs c√≠m')
                description = entry.get('summary', entry.get('description', ''))
                link = entry.get('link', '')
                
                # Publik√°l√°si id≈ë
                pub_date = None
                if hasattr(entry, 'published_parsed'):
                    pub_date = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed'):
                    pub_date = datetime(*entry.updated_parsed[:6])
                else:
                    pub_date = datetime.now()
                
                # Csak az elm√∫lt 48 √≥ra h√≠rei (korm√°nyzati elemz√©shez b≈ëvebb id≈ëablak)
                if datetime.now() - pub_date > timedelta(days=2):
                    continue
                
                # Ford√≠t√°s
                translated_title = translate_text(title)
                translated_description = translate_text(description)
                
                article = {
                    'id': generate_article_id(title, source['name']),
                    'original_title': title,
                    'title': translated_title,
                    'original_description': description,
                    'description': translated_description,
                    'source': source['name'],
                    'category': source['category'],
                    'link': link,
                    'pub_date': pub_date.isoformat(),
                    'timestamp': datetime.now().isoformat()
                }
                
                all_articles.append(article)
                
        except Exception as e:
            print(f"‚ùå Hiba {source['name']} feldolgoz√°sakor: {e}")
            continue
    
    print(f"\nüìä √ñsszesen {len(all_articles)} cikk √∂sszegy≈±jtve")
    
    # AI elemz√©s csak ha vannak cikkek
    if all_articles:
        print(f"\nü§ñ Korm√°nyzati AI elemz√©s ind√≠t√°sa...")
        processed_articles, executive_briefing = ai_analyzer.process_articles_for_government(all_articles)
        
        # Form√°zott cikkek el≈ëk√©sz√≠t√©se
        formatted_articles = [
            ai_analyzer.format_article_for_display(article)
            for article in processed_articles[:20]  # Top 20 cikk
        ]
        
        # Glob√°lis v√°ltoz√≥k friss√≠t√©se
        newsletter_data['articles'] = formatted_articles
        newsletter_data['executive_briefing'] = executive_briefing or "‚ö†Ô∏è Vezet≈ëi √∂sszefoglal√≥ nem el√©rhet≈ë - ellen≈ërizze az OpenAI API kulcsot."
    else:
        newsletter_data['articles'] = []
        newsletter_data['executive_briefing'] = "Nincs feldolgozhat√≥ cikk."
    
    newsletter_data['last_update'] = datetime.now().isoformat()
    newsletter_data['update_count'] += 1
    newsletter_data['processing_status'] = 'completed'
    
    print(f"\n‚úÖ Friss√≠t√©s k√©sz! Feldolgozott cikkek: {len(newsletter_data['articles'])}")
    print(f"{'='*60}\n")

def run_scheduler():
    """√útemez≈ë futtat√°sa h√°tt√©rsz√°lon"""
    while True:
        schedule.run_pending()
        time.sleep(60)

# √útemezett feladatok be√°ll√≠t√°sa
schedule.every(6).hours.do(fetch_and_process_news)

# Els≈ë futtat√°s h√°tt√©rsz√°lban 2 m√°sodperc m√∫lva
def delayed_first_run():
    """K√©sleltetett els≈ë futtat√°s"""
    time.sleep(2)
    print("\nüöÄ Els≈ë h√≠rek bet√∂lt√©se indul...")
    fetch_and_process_news()

# Els≈ë futtat√°s h√°tt√©rsz√°lban
first_run_thread = threading.Thread(target=delayed_first_run, daemon=True)
first_run_thread.start()

# √útemez≈ë ind√≠t√°sa h√°tt√©rsz√°lon
scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
scheduler_thread.start()

@app.route('/')
def index():
    """F≈ëoldal"""
    return render_template('index.html', data=newsletter_data)

@app.route('/api/articles')
def get_articles():
    """API v√©gpont a cikkek lek√©r√©s√©hez"""
    return jsonify(newsletter_data)

@app.route('/api/refresh', methods=['POST'])
def refresh_articles():
    """Manu√°lis friss√≠t√©s"""
    fetch_and_process_news()
    return jsonify({'success': True, 'message': 'H√≠rek friss√≠tve'})

@app.route('/api/rss-sources')
def get_rss_sources():
    """RSS forr√°sok √©s cikkeik VAL√ìS IDEJ≈∞ lek√©r√©se"""
    sources_with_articles = []
    
    for source in ECONOMIC_SOURCES:
        try:
            # Val√≥s id≈ëben lek√©rj√ºk az RSS feed-et
            feed = feedparser.parse(source['url'])
            recent_articles = []
            
            for entry in feed.entries[:3]:  # Max 3 legfrissebb cikk
                title = entry.get('title', 'Nincs c√≠m')
                link = entry.get('link', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # Publik√°l√°si id≈ë
                pub_date = None
                if hasattr(entry, 'published_parsed'):
                    pub_date = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed'):
                    pub_date = datetime(*entry.updated_parsed[:6])
                else:
                    pub_date = datetime.now()
                
                recent_articles.append({
                    'title': title,
                    'link': link,
                    'pub_date': pub_date.isoformat(),
                    'description': description[:150] + '...' if description else ''
                })
            
            source_info = {
                'name': source['name'],
                'url': source['url'],
                'category': source['category'],
                'articles': recent_articles
            }
            sources_with_articles.append(source_info)
            
        except Exception as e:
            print(f"‚ùå RSS lek√©r√©si hiba {source['name']}: {e}")
            # Fallback √ºres cikkekkel
            source_info = {
                'name': source['name'],
                'url': source['url'], 
                'category': source['category'],
                'articles': []
            }
            sources_with_articles.append(source_info)
    
    return jsonify({'sources': sources_with_articles})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)